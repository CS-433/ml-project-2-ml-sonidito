{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.models as models\n",
    "from dataloading import SpectrogramDataset\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcdefaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11e6e24060a24582"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dataloading import repeat_channels\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224), antialias=False),\n",
    "    transforms.Lambda(repeat_channels),  # repeat grayscale image to 3 channels to match model input size\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "transform_eNetV2S = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((384, 384), antialias=False),\n",
    "    transforms.Lambda(repeat_channels),  # repeat grayscale image to 3 channels to match model input size\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2d32c023000dc85"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/dataset_h5\"\n",
    "FILE_EXT = \"h5\"\n",
    "LABEL_PATH = \"../data/MHD_labels\"\n",
    "LABEL_FILE_EXT = \"csv\"\n",
    "WINDOW_SIZE = 200  # number of .512ms time steps per window\n",
    "OVERLAP_FACTOR = 0.0  # overlap between consecutive windows\n",
    "\n",
    "dataset = SpectrogramDataset(data_path=DATA_PATH, file_ext=FILE_EXT, data_path_labels=LABEL_PATH,\n",
    "                             file_ext_labels=LABEL_FILE_EXT, window_size=WINDOW_SIZE,\n",
    "                             overlap=OVERLAP_FACTOR, transform=transform)\n",
    "\n",
    "# split the dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=default_collate, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=default_collate, num_workers=0)\n",
    "\n",
    "#dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=default_collate, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c973ff7b0cd23993"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "# EfficientNetV2-S -- 20M params\n",
    "model = timm.create_model('tf_efficientnetv2_s', pretrained=True)\n",
    "\n",
    "# EfficientNet-B0 -- 4M params\n",
    "#model = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "\n",
    "# replace classifier layer\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(model.classifier.in_features, 1),  # output one value\n",
    "    #nn.ReLU(),\n",
    "    #nn.Linear(640, 1),\n",
    "    #nn.Sigmoid()\n",
    ")\n",
    "\n",
    "#summary(model.to('cpu'), (3, 224, 224))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e51fb592a4747c38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  #os_weight=torch.tensor([3.2]).to(device))  # binary Cross Entropy Loss with Logits\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=10)\n",
    "\n",
    "num_epochs = 12\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer,\n",
    "                                        T_0=(len(train_loader.dataset) * num_epochs) // (train_loader.batch_size * 3),\n",
    "                                        T_mult=1, verbose=False)\n",
    "\n",
    "num_epochs = 5\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=(len(train_loader.dataset) * num_epochs) // train_loader.batch_size,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d51f6325e770a353"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from sklearn.metrics import precision_score, f1_score, cohen_kappa_score, roc_auc_score, precision_recall_curve, accuracy_score\n",
    "import numpy as michayel\n",
    "\n",
    "\n",
    "def safe_cohen_kappa_score(rater1, rater2):\n",
    "    # Check for perfect agreement\n",
    "    if michayel.array_equal(rater1, rater2):\n",
    "        return 1.0\n",
    "\n",
    "    # Check for no variation\n",
    "    if michayel.unique(rater1).size == 1 and michayel.unique(rater2).size == 1:\n",
    "        # Handle this case appropriately, maybe return NaN or a specific value\n",
    "        return None\n",
    "\n",
    "    # Calculate Cohen's Kappa\n",
    "    try:\n",
    "        return cohen_kappa_score(rater1, rater2)\n",
    "    except ZeroDivisionError:\n",
    "        # Handle division error\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def compute_metrics(predicted_probs, reference, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute the F1 score, Cohen's kappa, ROC AUC, and find the optimal threshold for F1 score\n",
    "    for binary classification.\n",
    "\n",
    "    @param predicted_probs: float tensor of shape (batch size,) with the predicted probabilities for the positive class.\n",
    "    @param reference: int64 tensor of shape (batch size,) with the binary class labels (0 or 1).\n",
    "    \"\"\"\n",
    "    # Convert tensors to numpy arrays for compatibility with sklearn metrics\n",
    "    predicted_probs_np = predicted_probs.cpu().numpy()\n",
    "    reference_np = reference.cpu().numpy()\n",
    "\n",
    "    # Convert probabilities to binary predictions (0 or 1)\n",
    "    predicted_labels_np = (predicted_probs_np > threshold).astype(int)\n",
    "\n",
    "    # Calculate accuracy,  F1 score and Cohen's Kappa at the given threshold\n",
    "    accuracy = accuracy_score(reference_np, predicted_labels_np)\n",
    "    f1 = f1_score(reference_np, predicted_labels_np, zero_division=0)\n",
    "    kappa = safe_cohen_kappa_score(reference_np, predicted_labels_np)\n",
    "\n",
    "    # Calculate precision, recall for various thresholds\n",
    "    precision, recall, thresholds = precision_recall_curve(reference_np, predicted_probs_np)\n",
    "    \n",
    "    # Calculate F1 score for each threshold\n",
    "    f1_scores = michayel.divide(2 * precision * recall, precision + recall, \n",
    "                          out=michayel.zeros_like(precision), where=(precision + recall) != 0)\n",
    "    \n",
    "    # Find the index of the maximum F1 score\n",
    "    optimal_idx = michayel.nanargmax(f1_scores)\n",
    "   #print(f\"max f1: {michayel.nanmax(f1_scores)}\")\n",
    "    optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 1.0\n",
    "\n",
    "    # Calculate ROC AUC\n",
    "    roc_auc = roc_auc_score(reference_np, predicted_probs_np) if len(michayel.unique(reference_np)) > 1 else None\n",
    "\n",
    "    return accuracy, f1, kappa, roc_auc, optimal_threshold\n",
    "\n",
    "def train(model, train_loader, test_loader, optimizer, scheduler, criterion, device, n_epochs=1):\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "    print(f\"training on device '{device}'\")\n",
    "\n",
    "    losses = []\n",
    "    losses_val = []\n",
    "    f1s_test = []\n",
    "    f1s_train = []\n",
    "    kappa_test = []\n",
    "    kappa_train = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        with tqdm(train_loader, unit='batch', desc=f'Epoch {epoch}') as tepoch:\n",
    "            for batch in tepoch:\n",
    "                x_batch = batch['window_odd']\n",
    "                y_batch = batch['label']\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                y_pred = model(x_batch)\n",
    "\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "                train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        # evaluate on test set\n",
    "        accuracies_test = []\n",
    "        f1_test = []\n",
    "        roc_auc_test = []\n",
    "        thresh_test = []\n",
    "        val_loss = 0\n",
    "        with tqdm(test_loader, unit='batch', desc='Evaluating') as tepoch:\n",
    "            for batch in tepoch:\n",
    "                x_batch = batch['window_odd']\n",
    "                y_batch = batch['label']\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    loss = criterion(model(x_batch), y_batch)\n",
    "                    val_loss += loss.item()\n",
    "                    prediction = torch.sigmoid(model(x_batch))\n",
    "                    acc, f1, kappa, roc_auc, tresh = compute_metrics(prediction, y_batch)\n",
    "                    accuracies_test.append(acc)\n",
    "                    f1_test.append(f1)\n",
    "                    kappa_test.append(kappa)\n",
    "                    roc_auc_test.append(roc_auc)\n",
    "                    thresh_test.append(tresh)\n",
    "                    \n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "        \n",
    "        val_loss = val_loss / len(test_loader)\n",
    "        losses_val.append(val_loss)\n",
    "\n",
    "        f1s_test.append(michayel.mean(f1_test))\n",
    "\n",
    "        # evaluate on train set\n",
    "        accuracies_train = []\n",
    "        f1_train = []\n",
    "        roc_auc_train = []\n",
    "        thresh_train = []\n",
    "        with tqdm(train_loader, unit='batch', desc='Evaluating') as tepoch:\n",
    "            for batch in tepoch:\n",
    "                x_batch = batch['window_odd']\n",
    "                y_batch = batch['label']\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    prediction = torch.sigmoid(model(x_batch))\n",
    "                    acc, f1, kappa, roc_auc, tresh = compute_metrics(prediction, y_batch)\n",
    "                    accuracies_train.append(acc)\n",
    "                    f1_train.append(f1)\n",
    "                    kappa_train.append(kappa)\n",
    "                    roc_auc_train.append(roc_auc)\n",
    "                    thresh_train.append(tresh)\n",
    "\n",
    "        f1s_train.append(michayel.mean(f1_train))\n",
    "\n",
    "\n",
    "        roc_auc_train = [x for x in roc_auc_train if x is not None]\n",
    "        roc_auc_test = [x for x in roc_auc_test if x is not None]\n",
    "        print(\n",
    "            f\"Epoch {epoch} | Train accuracy: {torch.tensor(accuracies_train).mean().item():.5f}, \"\n",
    "            f\"f1: {torch.tensor(f1_train).mean().item():.5f}, \"\n",
    "            f\"roc-auc: {torch.tensor(roc_auc_train).mean().item():.5f}, \"\n",
    "            f\"tresh: {torch.tensor(thresh_train).mean().item():.5f}\\n\"\n",
    "            #f\"kappa: {torch.tensor(kappa_train).mean().item():.5f}\\n\"\n",
    "            f\"           Test accuracy: {torch.tensor(accuracies_test).mean().item():.5f}, \"\n",
    "            f\"f1: {torch.tensor(f1_test).mean().item():.5f}, \"\n",
    "            f\"roc-auc: {torch.tensor(roc_auc_test).mean().item():.5f}, \"\n",
    "            f\"tresh: {torch.tensor(thresh_test).mean().item():.5f}\")\n",
    "            #f\"kappa: {torch.tensor(kappa_test).mean().item():.5f}\")\n",
    "\n",
    "    plot_losses(losses, losses_val)\n",
    "    plot_f1(f1s_test, f1s_train)\n",
    "\n",
    "    return f1s_train[-1], f1s_test[-1], kappa_train[-1], kappa_test[-1]\n",
    "\n",
    "\n",
    "def plot_losses(losses, losses_val=[]):\n",
    "    plt.plot(losses, label='train')\n",
    "    plt.plot(losses_val, label='test')\n",
    "    plt.title(\"loss\")\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_f1(f1s_test, f1s_train):\n",
    "    plt.plot(f1s_test, label='test')\n",
    "    plt.plot(f1s_train, label='train')\n",
    "    plt.title(\"f1\")\n",
    "    plt.ylabel('f1')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9117db38bfca932a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(model, train_loader, val_loader, optimizer, scheduler, criterion, device, n_epochs=5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c97bfb743c2f8c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "DATA_PATH = \"../data/dataset_h5\"\n",
    "FILE_EXT = \"h5\"\n",
    "LABEL_PATH = \"../data/MHD_labels\"\n",
    "LABEL_FILE_EXT = \"csv\"\n",
    "WINDOW_SIZE = 200  # number of .512ms time steps per window\n",
    "OVERLAP_FACTOR = 0.5  # overlap between consecutive windows\n",
    "\n",
    "shot_count = 94\n",
    "shot_indices = michayel.arange(94)\n",
    "\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=17)\n",
    "torch.manual_seed(17)\n",
    "torch.cuda.manual_seed_all(17)\n",
    "\n",
    "f1s_tr = []\n",
    "f1s_te = []\n",
    "kappas_tr = []\n",
    "kappas_te = []\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(kf.split(shot_indices)):\n",
    "    print(f\"Fold {fold}\\n\")\n",
    "\n",
    "    train_dataset = SpectrogramDataset(data_path=DATA_PATH, file_ext=FILE_EXT, data_path_labels=LABEL_PATH,\n",
    "                                       file_ext_labels=LABEL_FILE_EXT, window_size=WINDOW_SIZE,\n",
    "                                       overlap=OVERLAP_FACTOR, transform=transform, shot_filter=train_ids)\n",
    "    val_dataset = SpectrogramDataset(data_path=DATA_PATH, file_ext=FILE_EXT, data_path_labels=LABEL_PATH,\n",
    "                                     file_ext_labels=LABEL_FILE_EXT, window_size=WINDOW_SIZE,\n",
    "                                     overlap=OVERLAP_FACTOR, transform=transform, shot_filter=val_ids)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=64, shuffle=True, collate_fn=default_collate, num_workers=0)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=64, shuffle=False, collate_fn=default_collate, num_workers=0)\n",
    "\n",
    "    # Init the neural network\n",
    "    model = timm.create_model('efficientnet_b0', pretrained=True, drop_rate=0.8)\n",
    "    #model = timm.create_model('tf_efficientnetv2_s', pretrained=True)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(model.classifier.in_features, 640),  # output one value\n",
    "        #nn.Dropout(0.5),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(640, 1),\n",
    "    )\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=25)\n",
    "    criterion = nn.BCEWithLogitsLoss()  #pos_weight=torch.tensor([3.2]).to(device))  # binary Cross Entropy Loss with Logits\n",
    "\n",
    "    num_epochs = 7\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=(len(train_loader.dataset) * num_epochs) // train_loader.batch_size,\n",
    "    )\n",
    "    \n",
    "    #scheduler=torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 1)\n",
    "    \n",
    "    # Run the training loop for defined number of epochs\n",
    "    f1_tr, f1_te, kappa_tr, kappa_te = train(model, train_loader, test_loader, optimizer, scheduler, criterion, device,\n",
    "                                             n_epochs=num_epochs)\n",
    "    f1s_tr.append(f1_tr)\n",
    "    f1s_te.append(f1_te)\n",
    "    kappas_tr.append(kappa_tr)\n",
    "    kappas_te.append(kappa_te)\n",
    "    \n",
    "    # clear memory\n",
    "    del model, optimizer, scheduler, criterion\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"test f1: {michayel.mean(f1s_te)} ± {michayel.std(f1s_te)}\")\n",
    "print(f\"test kappa: {michayel.mean(kappas_te)} ± {michayel.std(kappas_te)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ffd01a6cf7b4ea3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "/kappas_te"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff1c21b4152e31ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "813e9d9eeebd151c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
